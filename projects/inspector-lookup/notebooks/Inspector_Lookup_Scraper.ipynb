{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d869a7d2-be02-4eed-8311-5a351adc459b",
   "metadata": {},
   "source": [
    "# Inspector Lookup Scraper Project\n",
    "\n",
    "## Objective\n",
    "The goal of this project is to create a Python-based web scraping tool to automate the extraction of inspector data from the **Inspector Lookup** page of the Wisconsin Department of Safety and Professional Services website.\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "- Automates the selection of the \"Boiler and Unfired Pressure Vessels\" program.\n",
    "- Extracts inspector details:\n",
    "  - Inspector Name\n",
    "  - Program Area\n",
    "  - Email\n",
    "  - Phone Number\n",
    "  - Counties (one row per county)\n",
    "- Ensures compliance with web scraping best practices, including checking the website's `robots.txt` file.\n",
    "- Saves the extracted data into a clean, structured CSV file.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### 1. Import Libraries and Set Up Logging\n",
    "This step initializes the required libraries and sets up logging for better debugging and transparency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd51999-f2c1-4a97-a935-634237257651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463fde06-0106-4b4c-953d-b7cb59b0698b",
   "metadata": {},
   "source": [
    "### 2. Check `robots.txt` Compliance\n",
    "Ensure that the scraping activity is allowed by checking the website's `robots.txt` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab985f1-9232-484a-9d2a-f53090eb9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_robots_txt(base_url, user_agent):\n",
    "    robots_url = f\"{base_url}/robots.txt\"\n",
    "    logging.info(f\"Checking robots.txt at {robots_url}...\")\n",
    "    \n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(robots_url)\n",
    "    rp.read()\n",
    "    return rp.can_fetch(user_agent, base_url)\n",
    "\n",
    "# Base URL and User-Agent\n",
    "base_url = \"https://esla.wi.gov\"\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "# Check robots.txt\n",
    "if not check_robots_txt(base_url, user_agent):\n",
    "    logging.error(\"Scraping is not allowed by the robots.txt file. Exiting...\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a012f0-287c-4b77-887d-519f280b9ed6",
   "metadata": {},
   "source": [
    "### 3. Configure Selenium WebDriver\n",
    "Set up Selenium WebDriver with a `User-Agent` to mimic a real browser and run it in headless mode for efficient background execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f96885-9392-42d1-82aa-95c46792e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add User-Agent to mimic a browser\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "logging.info(\"Initializing WebDriver...\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75084e7d-1caa-46e1-9bff-621cefa0b006",
   "metadata": {},
   "source": [
    "### 4. Scraping the Inspector Lookup Page\n",
    "The script interacts with the dropdown menu, clicks the search button, and extracts the table data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b86409-ec2a-43d0-84d6-8efbef502ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target URL\n",
    "url = \"https://esla.wi.gov/inspectorlookup\"\n",
    "\n",
    "# Open the page\n",
    "logging.info(\"Opening the target URL...\")\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# Select the Program Area dropdown\n",
    "logging.info(\"Selecting 'Boiler and Unfired Pressure Vessels' from the dropdown...\")\n",
    "program_dropdown = Select(driver.find_element(By.ID, \"j_id0:j_id70:programArea\"))\n",
    "program_dropdown.select_by_visible_text(\"Boiler and Unfired Pressure Vessels\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Click Search\n",
    "logging.info(\"Clicking the 'Search' button...\")\n",
    "driver.find_element(By.CSS_SELECTOR, \".btn.btn-primary.searchButton\").click()\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab30dcb-4f77-4c1e-bcf0-90c0f5190d10",
   "metadata": {},
   "source": [
    "### 5. Save Data to a CSV File\n",
    "Save the extracted data into a CSV file. Each county listed in the table gets its own row for clean and structured output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b13fd-9f4e-4574-aeed-66e74f536243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare CSV file\n",
    "output_file = \"inspectors_expanded.csv\"\n",
    "logging.info(f\"Saving scraped data to {output_file}...\")\n",
    "\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Inspector Name\", \"Program Area\", \"Email\", \"Phone Number\", \"County\"])\n",
    "\n",
    "    # Scrape the table\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR, \".table.table-striped.no-footer.dataTable tbody tr\")\n",
    "    for row in rows:\n",
    "        cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        inspector_name = cols[0].text\n",
    "        program_area = cols[1].text\n",
    "\n",
    "        # Extract Contact Information (both spans)\n",
    "        contact_spans = cols[2].find_elements(By.TAG_NAME, \"span\")\n",
    "        email = contact_spans[0].text if len(contact_spans) > 0 else \"\"\n",
    "        phone = contact_spans[1].text if len(contact_spans) > 1 else \"\"\n",
    "\n",
    "        # Split the County column into individual counties\n",
    "        counties = cols[3].text.split(\", \")\n",
    "\n",
    "        # Create a row for each county\n",
    "        for county in counties:\n",
    "            writer.writerow([inspector_name, program_area, email, phone, county])\n",
    "            logging.info(f\"Added record for {inspector_name} in {county}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba58d3-cc53-40f7-b1e3-9ab43728dcfe",
   "metadata": {},
   "source": [
    "### 6. Clean Up\n",
    "Close the Selenium WebDriver after completing the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95136154-f0e2-44bf-826a-daba82abf216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close WebDriver\n",
    "driver.quit()\n",
    "logging.info(\"Web scraping completed successfully.\")\n",
    "\n",
    "print(f\"Data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ceaa36-8f43-45cd-b9a0-220024016ca1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "The resulting CSV file (`inspectors_expanded.csv`) contains the following columns:\n",
    "- Inspector Name\n",
    "- Program Area\n",
    "- Email\n",
    "- Phone Number\n",
    "- County (one row per county)\n",
    "\n",
    "### Sample Data:\n",
    "\n",
    "| Inspector Name   | Program Area                      | Email                           | Phone Number    | County    |\n",
    "|-------------------|-----------------------------------|---------------------------------|-----------------|-----------|\n",
    "| Dean Y   | Boiler and Unfired Pressure Vessels | dean.y@example.com     | (866) 123-4567  | Buffalo   |\n",
    "| Dean Y    | Boiler and Unfired Pressure Vessels | dean.y@example.com     | (866) 123-4567  | Jackson   |\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "This project demonstrates how to:\n",
    "1. Scrape dynamic content using Selenium.\n",
    "2. Respect `robots.txt` compliance and web scraping best practices.\n",
    "3. Save structured data to a CSV file for easy analysis and usage.\n",
    "\n",
    "This notebook showcases practical skills in web scraping, data organization, and Python programming. Let me know if you'd like to see more projects like this!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
